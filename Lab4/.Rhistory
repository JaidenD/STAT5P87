# alpha_i_new = alpha_i_old + y_i*y_j*(alpha_j_old - alpha_j_new)
alpha_j_old = alpha[j]
alpha_i_old = alpha[i]
alpha_j_new = alpha_j_old - y[j] * (E_i - E_j) / eta
C = 0
if (y[i] != y[j]) {
L = max(0, alpha_j_old - alpha_i_old)
H = min(0, alpha_j_old - alpha_i_old)
} else {
L = max(0, alpha_j_old + alpha_i_old)
H = min(0, alpha_j_old + alpha_i_old)
}
alpha_j_new = min(min(alpha_j_new, L), H)
alpha_i_new = alpha_i_old + y[i]*y[j]*(alpha_j_old - alpha_j_new)
alpha[j] = alpha_j_new
alpha[i] = alpha_i_new
}
}
# Recompute B, b0, and S after updating alpha
B = x %*% (y * alpha)
S = which(alpha > 0)
b0 = beta0(B, S, x, y)
}
return(list(alpha = alpha, B = B, b0 = b0, iterations = iter))
}
beta0 = function(B, S, x, y) {
# b0 = average(1/y[i] - x_i^T B) for i in S (support vectors)
if (length(S) == 0) {
return(0)  # fallback if no support vectors
}
b0_sum = 0
for (i in S) {
b0_sum = b0_sum + (1 / y[i] - as.numeric(t(x[, i]) %*% B))
}
return(b0_sum / length(S))
}
# ---------------------------------------------------------------
# Test Case
# ---------------------------------------------------------------
set.seed(1)
# Create a test data: 2 features and 6 samples.
# Each column of x is a sample. Here, x is a 2 x 6 matrix.
x = matrix(c(
1, 2,   # sample 1
2, 3,   # sample 2
3, 3,   # sample 3
4, 3,   # sample 4
4, 5,   # sample 5
6, 5    # sample 6
), nrow = 2, ncol = 6, byrow = FALSE)
# Labels: first three samples are +1, the last three are -1
y = c(1, 1, 1, -1, -1, -1)
# Run SMO with a stricter threshold (1e-4) to force more iterations
res = SMO(x, y, threshold = 1e-4, max_iter = 1000)
SMO = function(x, y, threshold = 1e-2, max_iter = 1000) {
# x:  d x n  matrix (d features, n samples)
# y:  length-n vector of labels (+1 or -1)
n = ncol(x)
# initial alpha
alpha = rep(1/n, n)
# Initial B and b0
B = x %*% (y * alpha)
S = which(alpha > 0)
b0 = beta0(B, S, x, y)
iter = 0
# Make alpha_old arbitrarily large to enter the for loop
alpha_old = Inf
while (max(abs(alpha_old - alpha)) > threshold && iter < max_iter) {
print(max(abs(alpha_old - alpha)))
iter = iter + 1
alpha_old = alpha
# Double loop over pairs (i, j)
for (i in 1:n) {
for (j in 1:n) {
if (j == i) next
# Compute errors for samples i and j
E_i = as.numeric(t(x[, i]) %*% B) - b0 - y[i]
E_j = as.numeric(t(x[, j]) %*% B) - b0 - y[j]
# Compute eta = K(ii) + K(jj) - 2K(ij) for linear kernel
eta = as.numeric(
t(x[, i]) %*% x[, i] +
t(x[, j]) %*% x[, j] -
2 * (t(x[, i]) %*% x[, j])
)
if (abs(eta) < 1e-12) {
# Avoid division by zero
next
}
# Standard SMO update:
# alpha_j_new = alpha_j_old - y_j*(E_i - E_j)/eta
# alpha_i_new = alpha_i_old + y_i*y_j*(alpha_j_old - alpha_j_new)
alpha_j_old = alpha[j]
alpha_i_old = alpha[i]
alpha_j_new = alpha_j_old - y[j] * (E_i - E_j) / eta
C = 0
if (y[i] != y[j]) {
L = max(0, alpha_j_old - alpha_i_old)
H = min(0, alpha_j_old - alpha_i_old)
} else {
L = max(0, alpha_j_old + alpha_i_old)
H = min(0, alpha_j_old + alpha_i_old)
}
alpha_j_new = min(max(alpha_j_new, L), H)
alpha_i_new = alpha_i_old + y[i]*y[j]*(alpha_j_old - alpha_j_new)
alpha[j] = alpha_j_new
alpha[i] = alpha_i_new
}
}
# Recompute B, b0, and S after updating alpha
B = x %*% (y * alpha)
S = which(alpha > 0)
b0 = beta0(B, S, x, y)
}
return(list(alpha = alpha, B = B, b0 = b0, iterations = iter))
}
beta0 = function(B, S, x, y) {
# b0 = average(1/y[i] - x_i^T B) for i in S (support vectors)
if (length(S) == 0) {
return(0)  # fallback if no support vectors
}
b0_sum = 0
for (i in S) {
b0_sum = b0_sum + (1 / y[i] - as.numeric(t(x[, i]) %*% B))
}
return(b0_sum / length(S))
}
# ---------------------------------------------------------------
# Test Case
# ---------------------------------------------------------------
set.seed(1)
# Create a test data: 2 features and 6 samples.
# Each column of x is a sample. Here, x is a 2 x 6 matrix.
x = matrix(c(
1, 2,   # sample 1
2, 3,   # sample 2
3, 3,   # sample 3
4, 3,   # sample 4
4, 5,   # sample 5
6, 5    # sample 6
), nrow = 2, ncol = 6, byrow = FALSE)
# Labels: first three samples are +1, the last three are -1
y = c(1, 1, 1, -1, -1, -1)
# Run SMO with a stricter threshold (1e-4) to force more iterations
res = SMO(x, y, threshold = 1e-4, max_iter = 1000)
cat("Final alpha:\n", res$alpha, "\n")
cat("Weight vector B:\n", res$B, "\n")
cat("Bias b0:\n", res$b0, "\n")
cat("Number of iterations:\n", res$iterations, "\n")
SMO = function(x, y, threshold = 1e-2, max_iter = 1000) {
# x:  d x n  matrix (d features, n samples)
# y:  length-n vector of labels (+1 or -1)
n = ncol(x)
# initial alpha
alpha = rep(1/n, n)
# Initial B and b0
B = x %*% (y * alpha)
S = which(alpha > 0)
b0 = beta0(B, S, x, y)
iter = 0
# Make alpha_old arbitrarily large to enter the for loop
alpha_old = Inf
while (max(abs(alpha_old - alpha)) > threshold && iter < max_iter) {
print(max(abs(alpha_old - alpha)))
iter = iter + 1
alpha_old = alpha
# Double loop over pairs (i, j)
for (i in 1:n) {
for (j in 1:n) {
if (j == i) next
# Compute errors for samples i and j
E_i = as.numeric(t(x[, i]) %*% B) - b0 - y[i]
E_j = as.numeric(t(x[, j]) %*% B) - b0 - y[j]
# Compute eta = K(ii) + K(jj) - 2K(ij) for linear kernel
eta = as.numeric(
t(x[, i]) %*% x[, i] +
t(x[, j]) %*% x[, j] -
2 * (t(x[, i]) %*% x[, j])
)
if (abs(eta) < 1e-12) {
# Avoid division by zero
next
}
# Standard SMO update:
# alpha_j_new = alpha_j_old - y_j*(E_i - E_j)/eta
# alpha_i_new = alpha_i_old + y_i*y_j*(alpha_j_old - alpha_j_new)
alpha_j_old = alpha[j]
alpha_i_old = alpha[i]
alpha_j_new = alpha_j_old - y[j] * (E_i - E_j) / eta
C = 0
if (y[i] != y[j]) {
L = max(0, alpha_j_old - alpha_i_old)
H = min(0, alpha_j_old - alpha_i_old)
} else {
L = max(0, alpha_j_old + alpha_i_old)
H = min(0, alpha_j_old + alpha_i_old)
}
alpha_j_new = max(alpha_j_new, L)
alpha_i_new = alpha_i_old + y[i]*y[j]*(alpha_j_old - alpha_j_new)
alpha[j] = alpha_j_new
alpha[i] = alpha_i_new
}
}
# Recompute B, b0, and S after updating alpha
B = x %*% (y * alpha)
S = which(alpha > 0)
b0 = beta0(B, S, x, y)
}
return(list(alpha = alpha, B = B, b0 = b0, iterations = iter))
}
beta0 = function(B, S, x, y) {
# b0 = average(1/y[i] - x_i^T B) for i in S (support vectors)
if (length(S) == 0) {
return(0)  # fallback if no support vectors
}
b0_sum = 0
for (i in S) {
b0_sum = b0_sum + (1 / y[i] - as.numeric(t(x[, i]) %*% B))
}
return(b0_sum / length(S))
}
# ---------------------------------------------------------------
# Test Case
# ---------------------------------------------------------------
set.seed(1)
# Create a test data: 2 features and 6 samples.
# Each column of x is a sample. Here, x is a 2 x 6 matrix.
x = matrix(c(
1, 2,   # sample 1
2, 3,   # sample 2
3, 3,   # sample 3
4, 3,   # sample 4
4, 5,   # sample 5
6, 5    # sample 6
), nrow = 2, ncol = 6, byrow = FALSE)
# Labels: first three samples are +1, the last three are -1
y = c(1, 1, 1, -1, -1, -1)
# Run SMO with a stricter threshold (1e-4) to force more iterations
res = SMO(x, y, threshold = 1e-4, max_iter = 1000)
SMO = function(x, y, threshold = 1e-2, max_iter = 1000) {
# x:  d x n  matrix (d features, n samples)
# y:  length-n vector of labels (+1 or -1)
n = ncol(x)
# initial alpha
alpha = rep(1/n, n)
# Initial B and b0
B = x %*% (y * alpha)
S = which(alpha > 0)
b0 = beta0(B, S, x, y)
iter = 0
# Make alpha_old arbitrarily large to enter the for loop
alpha_old = Inf
while (max(abs(alpha_old - alpha)) > threshold && iter < max_iter) {
print(max(abs(alpha_old - alpha)))
iter = iter + 1
alpha_old = alpha
# Double loop over pairs (i, j)
for (i in 1:n) {
for (j in 1:n) {
if (j == i) next
# Compute errors for samples i and j
E_i = as.numeric(t(x[, i]) %*% B) - b0 - y[i]
E_j = as.numeric(t(x[, j]) %*% B) - b0 - y[j]
# Compute eta = K(ii) + K(jj) - 2K(ij) for linear kernel
eta = as.numeric(
t(x[, i]) %*% x[, i] +
t(x[, j]) %*% x[, j] -
2 * (t(x[, i]) %*% x[, j])
)
if (abs(eta) < 1e-12) {
# Avoid division by zero
next
}
# Standard SMO update:
# alpha_j_new = alpha_j_old - y_j*(E_i - E_j)/eta
# alpha_i_new = alpha_i_old + y_i*y_j*(alpha_j_old - alpha_j_new)
alpha_j_old = alpha[j]
alpha_i_old = alpha[i]
alpha_j_new = alpha_j_old - y[j] * (E_i - E_j) / eta
C = 0
if (y[i] != y[j]) {
L = max(0, alpha_j_old - alpha_i_old)
H = min(0, alpha_j_old - alpha_i_old)
} else {
L = max(0, alpha_j_old + alpha_i_old)
H = min(0, alpha_j_old + alpha_i_old)
}
alpha_j_new = min(alpha_j_new, L)
alpha_i_new = alpha_i_old + y[i]*y[j]*(alpha_j_old - alpha_j_new)
alpha[j] = alpha_j_new
alpha[i] = alpha_i_new
}
}
# Recompute B, b0, and S after updating alpha
B = x %*% (y * alpha)
S = which(alpha > 0)
b0 = beta0(B, S, x, y)
}
return(list(alpha = alpha, B = B, b0 = b0, iterations = iter))
}
beta0 = function(B, S, x, y) {
# b0 = average(1/y[i] - x_i^T B) for i in S (support vectors)
if (length(S) == 0) {
return(0)  # fallback if no support vectors
}
b0_sum = 0
for (i in S) {
b0_sum = b0_sum + (1 / y[i] - as.numeric(t(x[, i]) %*% B))
}
return(b0_sum / length(S))
}
# ---------------------------------------------------------------
# Test Case
# ---------------------------------------------------------------
set.seed(1)
# Create a test data: 2 features and 6 samples.
# Each column of x is a sample. Here, x is a 2 x 6 matrix.
x = matrix(c(
1, 2,   # sample 1
2, 3,   # sample 2
3, 3,   # sample 3
4, 3,   # sample 4
4, 5,   # sample 5
6, 5    # sample 6
), nrow = 2, ncol = 6, byrow = FALSE)
# Labels: first three samples are +1, the last three are -1
y = c(1, 1, 1, -1, -1, -1)
# Run SMO with a stricter threshold (1e-4) to force more iterations
res = SMO(x, y, threshold = 1e-4, max_iter = 1000)
SMO = function(x, y, threshold = 1e-2, max_iter = 1000) {
# x:  d x n  matrix (d features, n samples)
# y:  length-n vector of labels (+1 or -1)
n = ncol(x)
# initial alpha
alpha = rep(1/n, n)
# Initial B and b0
B = x %*% (y * alpha)
S = which(alpha > 0)
b0 = beta0(B, S, x, y)
iter = 0
# Make alpha_old arbitrarily large to enter the for loop
alpha_old = Inf
while (max(abs(alpha_old - alpha)) > threshold && iter < max_iter) {
print(max(abs(alpha_old - alpha)))
iter = iter + 1
alpha_old = alpha
# Double loop over pairs (i, j)
for (i in 1:n) {
for (j in 1:n) {
if (j == i) next
# Compute errors for samples i and j
E_i = as.numeric(t(x[, i]) %*% B) - b0 - y[i]
E_j = as.numeric(t(x[, j]) %*% B) - b0 - y[j]
# Compute eta = K(ii) + K(jj) - 2K(ij) for linear kernel
eta = as.numeric(
t(x[, i]) %*% x[, i] +
t(x[, j]) %*% x[, j] -
2 * (t(x[, i]) %*% x[, j])
)
if (abs(eta) < 1e-12) {
# Avoid division by zero
next
}
# Standard SMO update:
# alpha_j_new = alpha_j_old - y_j*(E_i - E_j)/eta
# alpha_i_new = alpha_i_old + y_i*y_j*(alpha_j_old - alpha_j_new)
alpha_j_old = alpha[j]
alpha_i_old = alpha[i]
alpha_j_new = alpha_j_old - y[j] * (E_i - E_j) / eta
C = 0
if (y[i] != y[j]) {
L = max(0, alpha_j_old - alpha_i_old)
H = min(0, alpha_j_old - alpha_i_old)
} else {
L = max(0, alpha_j_old + alpha_i_old)
H = min(0, alpha_j_old + alpha_i_old)
}
alpha_j_new = min(max(alpha_j_new, L), H)
alpha_i_new = alpha_i_old + y[i]*y[j]*(alpha_j_old - alpha_j_new)
alpha[j] = alpha_j_new
alpha[i] = alpha_i_new
}
}
# Recompute B, b0, and S after updating alpha
B = x %*% (y * alpha)
S = which(alpha > 0)
b0 = beta0(B, S, x, y)
}
return(list(alpha = alpha, B = B, b0 = b0, iterations = iter))
}
beta0 = function(B, S, x, y) {
# b0 = average(1/y[i] - x_i^T B) for i in S (support vectors)
if (length(S) == 0) {
return(0)  # fallback if no support vectors
}
b0_sum = 0
for (i in S) {
b0_sum = b0_sum + (1 / y[i] - as.numeric(t(x[, i]) %*% B))
}
return(b0_sum / length(S))
}
# ---------------------------------------------------------------
# Test Case
# ---------------------------------------------------------------
set.seed(1)
# Create a test data: 2 features and 6 samples.
# Each column of x is a sample. Here, x is a 2 x 6 matrix.
x = matrix(c(
1, 2,   # sample 1
2, 3,   # sample 2
3, 3,   # sample 3
4, 3,   # sample 4
4, 5,   # sample 5
6, 5    # sample 6
), nrow = 2, ncol = 6, byrow = FALSE)
# Labels: first three samples are +1, the last three are -1
y = c(1, 1, 1, -1, -1, -1)
# Run SMO with a stricter threshold (1e-4) to force more iterations
res = SMO(x, y, threshold = 1e-4, max_iter = 1000)
cat("Final alpha:\n", res$alpha, "\n")
cat("Weight vector B:\n", res$B, "\n")
cat("Bias b0:\n", res$b0, "\n")
cat("Number of iterations:\n", res$iterations, "\n")
SMO = function(x, y, threshold = 1e-2, max_iter = 1000) {
# x:  d x n  matrix (d features, n samples)
# y:  length-n vector of labels (+1 or -1)
n = ncol(x)
# initial alpha
alpha = rep(1/n, n)
# Initial B and b0
B = x %*% (y * alpha)
S = which(alpha > 0)
b0 = beta0(B, S, x, y)
iter = 0
# Make alpha_old arbitrarily large to enter the for loop
alpha_old = Inf
while (max(abs(alpha_old - alpha)) > threshold && iter < max_iter) {
print(max(abs(alpha_old - alpha)))
iter = iter + 1
alpha_old = alpha
# Double loop over pairs (i, j)
for (i in 1:n) {
for (j in 1:n) {
if (j == i) next
# Compute errors for samples i and j
E_i = as.numeric(t(x[, i]) %*% B) - b0 - y[i]
E_j = as.numeric(t(x[, j]) %*% B) - b0 - y[j]
# Compute eta = K(ii) + K(jj) - 2K(ij) for linear kernel
eta = as.numeric(
t(x[, i]) %*% x[, i] +
t(x[, j]) %*% x[, j] -
2 * (t(x[, i]) %*% x[, j])
)
if (abs(eta) < 1e-12) {
# Avoid division by zero
next
}
# Standard SMO update:
# alpha_j_new = alpha_j_old - y_j*(E_i - E_j)/eta
# alpha_i_new = alpha_i_old + y_i*y_j*(alpha_j_old - alpha_j_new)
alpha_j_old = alpha[j]
alpha_i_old = alpha[i]
alpha_j_new = alpha_j_old - y[j] * (E_i - E_j) / eta
C = 0
if (y[i] != y[j]) {
L = max(0, alpha_j_old - alpha_i_old)
H = min(0, alpha_j_old - alpha_i_old)
} else {
L = max(0, alpha_j_old + alpha_i_old)
H = min(0, alpha_j_old + alpha_i_old)
}
alpha_j_new = min(alpha_j_new, H)
alpha_i_new = alpha_i_old + y[i]*y[j]*(alpha_j_old - alpha_j_new)
alpha[j] = alpha_j_new
alpha[i] = alpha_i_new
}
}
# Recompute B, b0, and S after updating alpha
B = x %*% (y * alpha)
S = which(alpha > 0)
b0 = beta0(B, S, x, y)
}
return(list(alpha = alpha, B = B, b0 = b0, iterations = iter))
}
beta0 = function(B, S, x, y) {
# b0 = average(1/y[i] - x_i^T B) for i in S (support vectors)
if (length(S) == 0) {
return(0)  # fallback if no support vectors
}
b0_sum = 0
for (i in S) {
b0_sum = b0_sum + (1 / y[i] - as.numeric(t(x[, i]) %*% B))
}
return(b0_sum / length(S))
}
# ---------------------------------------------------------------
# Test Case
# ---------------------------------------------------------------
set.seed(1)
# Create a test data: 2 features and 6 samples.
# Each column of x is a sample. Here, x is a 2 x 6 matrix.
x = matrix(c(
1, 2,   # sample 1
2, 3,   # sample 2
3, 3,   # sample 3
4, 3,   # sample 4
4, 5,   # sample 5
6, 5    # sample 6
), nrow = 2, ncol = 6, byrow = FALSE)
# Labels: first three samples are +1, the last three are -1
y = c(1, 1, 1, -1, -1, -1)
# Run SMO with a stricter threshold (1e-4) to force more iterations
res = SMO(x, y, threshold = 1e-4, max_iter = 1000)
